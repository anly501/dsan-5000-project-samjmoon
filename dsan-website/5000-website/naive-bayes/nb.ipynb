{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Naïve Bayes\"\n",
    "format:\n",
    "    html:\n",
    "        page-layout: full\n",
    "        code-fold: show\n",
    "        code-copy: true\n",
    "        code-tools: true\n",
    "        code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a machine learning technique designed to solve classification problems. It relies on Bayes’ theorem, which is used to determine conditional probability of events. Naive Bayes assumes that all features used for classification are independent of each other.\n",
    "\n",
    "<div style=\"text-align:center; font-size:larger;\">\n",
    " $P(C_k | X) = \\frac{P(C_k) \\cdot P(X|C_k)}{P(X)}$\n",
    "</div>\n",
    "\n",
    "In the formula, we recognize variables $C_k$ as a class, and $X$ as a feature. $P(C_k | X)$ is the posterior probability. $P(C_k)$ is the class prior probability. $P(X | C_k)$ is the likelihood of observing features $X$ given $C_k$ is true. $P(X)$ is the feature prior probability. This formula calculates the probability of a class given a feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental goals of the Naive Bayes algorithm are classifying data into distinct categories and delivering probabilistic predictions. This is accomplished by assigning class labels to data points grounded in the likelihood of each point belonging to a particular class, incorporating informed decision-making based on past knowledge. Naive Bayes exhibits versatility in accommodating various feature types, including text, continuous variables, or binary data. It also employs Laplace smoothing as a safeguard against the occurrence of zero probabilities in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants of Naive Bayes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**<br>\n",
    "- Suitable for continuous data with a Gaussian (normal) distribution. It is commonly used in real-valued datasets where features are assumed to be continuous and follow a bell-shaped curve. <br>\n",
    "- Example: Medical diagnosis, where attributes such as blood pressure, height, or weight are measured continuously and assumed to be normally distributed. <br>\n",
    "\n",
    "**Multinomial Naive Bayes**<br>\n",
    "- Fitting for text and categorical data. Common in natural language processing (NLP) for text classification tasks. <br>\n",
    "- Example: Spam email detection, sentiment analysis, and document categorization <br>\n",
    "\n",
    "**Bernoulli Naive Bayes**<br>\n",
    "- Appropriate for binary data, where features are binary variables that represent the presence or absence of certain attributes. <br>\n",
    "- Example: Facial recognition systems, where the presence or absence of specific facial features is crucial for classification. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
